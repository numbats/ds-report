# Topic Modelling {#sec-topicmod}

As mentioned in Section @sec-unit-bigram, since the collected university data is relatively small, to make more educated and meaningful interpretations, similar words shall be grouped together and counted by groups. This is usually computed using text corpus, which is a language resource consisting of a large and structured set of texts, since data science is a new term waiting to be defined, there is no available text corpus on this topic. Therefore, we adopted the concept of word embedding models and tried to build our own text corpus.

There are multiple publicly available models and packages to conduct similar computations, such as `word2vec` and `text2vec`, however, each model takes hours to fit. Due to time constrains, we have only fitted the Dirichlet Allocation (LDA) model with a few parameter adjustments using the `text2vec` package with the concepts illustrated by @das2016data. 

## Algorithm and Model Fitting {#sec-model}

According to @das2016data, the algorithm behind the LDA model is to convert words to document-term matrix (DTM), where the rows, columns and entries correspond to documents, terms and counts respectively. LDA then fits a probabilistic model that assumes a mixture of latent topics, where each topic has a multinomial distribution for all words. The number of topics is a parameter that could be adjusted by needs.

The model must be trained before it could be used, we web scraped 4448 Wikipedia articles as training data, including 2816 articles in statistics, 1005 articles in sociology and 627 in computing. The initial codes and functions to build the LDA model was provided by Dr Tanaka,  we have tested model outputs using different number of topics and tried out training the LDA models with different combinations of data.

```{r setup, include = FALSE}
library(tidyverse)
library(kableExtra)
library(patchwork)

knitr::opts_chunk$set(echo = FALSE,
                      warning = FALSE,
                      message= FALSE)
theme_set(theme_minimal())

rmv_col <- function(x){
  read_csv(x) %>% 
  select(-`...1`)
}

colors <- RColorBrewer::brewer.pal(10, "Paired")
names(colors) <- levels(factor(1:10))
my_scale <- scale_fill_manual(name = "topic", values = colors)  

data_s <- rmv_col("data/stats.csv")
data_ssc <- rmv_col("data/ssc.csv")
data_cs <- rmv_col("data/cs.csv")
data_comp <- rmv_col("data/comp.csv")
mod_sum <- rmv_col("data/modelsum_ssc.csv")
```

The table below shows a glimpse of the model output of the initial LDA model, after training by data collected from all 4448 Wikipedia articles on 12 topics.

```{r}
mod_sum %>% 
  head(120) %>% 
  kable(align = "c") %>% 
  kable_styling(bootstrap_options = "condensed",
                position = "center") %>% 
  scroll_box(height = "420px")
```

Term shows all the word extracted from the training data (Wikipedia articles), as each latent topic has a multinomial distribution for all words, beta value of a term is the score / probability computed for that particular topic. Highest beta value indicates highest probability, which means the term is most likely belongs to the corresponding topic.


## Model Adjustments

The results provided differs from models, @fig-ssc-cs compares the results produced by the full model and model without sociology data on ten topics.



```{r}
ssc <- data_ssc %>% 
  mutate(topic = as_factor(topic)) %>% 
  group_by(School) %>% 
  count(topic, sort = TRUE) %>%
  mutate(prop = round(n/sum(n),4)) %>% 
  ungroup() %>% 
  mutate(topic = fct_reorder(topic, prop, sum))%>% 
  ggplot(aes(x = prop,
             y = topic,
             fill = topic))+
  geom_col() +
  my_scale +
  facet_wrap(~School, scales = "free_y") +
  theme(legend.position = "none") +
  labs(x = "proportion",
       title = "Results of the Full Model")

cs <- data_cs %>% 
  mutate(topic = as_factor(topic)) %>% 
  group_by(School) %>% 
  count(topic, sort = TRUE) %>%
  mutate(prop = round(n/sum(n),4)) %>% 
  ungroup() %>% 
  mutate(topic = fct_reorder(topic, prop, sum))%>% 
  ggplot(aes(x = prop,
             y = topic,
             fill = topic))+
  geom_col() +
  my_scale +
  facet_wrap(~School, scales = "free_y") +
  theme(legend.position = "none") +
  labs(x = "proportion",
       y = "",
       title = "Results of Model Without Sociology Data")
```

```{r fig-ssc-cs, fig.cap = "Compare Results Between Full Model and Without Sociology Data", fig.width=10}
ssc + cs
```

From the results computed by the full model, Topics 9, 10, 2 and 8 occupies relatively higher proportion compare with the others, but the order varies across universities, and their proportions are not significantly larger than the rest of other topics, makes it hard to draw meaningful interpretations. On the right hand side, results from the model without sociology data demonstrates a better picture: Topics 6, 8 and 9 in this case are the top 3 topics across all Go8 universities, however, proportions of Topic 10, 5, 7, 3 and 4 are also obvious higher in some of the universities, brings in difficulties to make justifications.     

As sociology data tends to brings in noises to the model, and is not closely relevant to the data science topic compare with statistics and computing, another two models are fitted using only statistics data and computing data respectively, the results of both models are shown in @fig-comp-stats.

```{r fig.width=10}
comp <- data_comp %>% 
  mutate(topic = as_factor(topic)) %>% 
  group_by(School) %>% 
  count(topic, sort = TRUE) %>%
  mutate(prop = round(n/sum(n),4)) %>% 
  ungroup() %>% 
  mutate(topic = fct_reorder(topic, prop, sum))%>% 
  ggplot(aes(x = prop,
             y = topic,
             fill = topic))+
  geom_col() +
  my_scale +
  facet_wrap(~School, scales = "free_y") +
  theme(legend.position = "none") +
  labs(x = "proportion",
       title = "Results of Model Using Only Comp Data")

stats <- data_s %>% 
  mutate(topic = as_factor(topic)) %>% 
  group_by(School) %>% 
  count(topic, sort = TRUE) %>%
  mutate(prop = round(n/sum(n),4)) %>% 
  ungroup() %>% 
  mutate(topic = fct_reorder(topic, prop, sum))%>% 
  ggplot(aes(x = prop,
             y = topic,
             fill = topic))+
  geom_col() +
  my_scale +
  facet_wrap(~School, scales = "free_y") +
  theme(legend.position = "none") +
  labs(x = "proportion",
       y = "",
       title = "Results of Model Using Only Stats Data")
```

```{r fig-comp-stats, fig.cap = "Compare Results Between Full Model and Without Sociology Data", fig.width=10}
comp + stats
```

Topic 2 is the only dominating topic based on the results provided by the model using only computing data, which provides a clearer picture than the previous models. The table below listed the top ten words for each topics, it turns out Topic 2 contains words like comput (computation etc.), system, program, softwar (software), which are associated with computational aspects, especially software. This model provides a more meaningful results than the prior ones, however, there is not much interpretations could be made for the other topics, the information it offers is still not very satisfying.

```{r}
rmv_col("data/term_comp30.csv") %>% 
  kable() %>% 
  kable_styling(bootstrap_options = "condensed") %>% 
  scroll_box(width = "1250px")
```

In terms of the model trained by only data in statistics, there are also dominating topics across all eight universities: Topics 6 and 1, besides, topics 5, 10, 2, 9 together took a relatively higher proportion compare with the rest of other topics. Both models using only computing or statistical data delivers better results, model trained by only statistical data provides more information than the other, hence is selected to use for further analysis on our university data.

Note that it requires highly skilled linguists and huge efforts to establish a proper text corpus, the model we built is still fairly basic and could be further optimised by adjustments.
 
