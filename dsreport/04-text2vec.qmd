# Topic Modelling {#sec-topicmod}

As mentioned in Section @sec-unit-bigram, since the collected university data is relatively small, to make more educated and meaningful interpretations, similar words shall be grouped together and counted by groups. This is usually computed using text corpus, which is a language resource consisting of a large and structured set of texts, since data science is a new term waiting to be defined, there is no available text corpus on this topic. Therefore, we adopted the concept of word embedding models and tried to build our own text corpus.

There are multiple publicly available models and packages to conduct similar computations, such as `word2vec` and `text2vec`, however, each model takes hours to fit. Due to time constrains, we have only fitted the Dirichlet Allocation (LDA) model with a few parameter adjustments using the `text2vec` package with the concepts illustrated by @das2016data. 

## Algorithm and Model Fitting {#sec-model}

According to @das2016data, the algorithm behind the LDA model is to convert words to document-term matrix (DTM), where the rows, columns and entries correspond to documents, terms and counts respectively. LDA then fits a probabilistic model that assumes a mixture of latent topics, where each topic has a multinomial distribution for all words. The number of topics is a parameter that could be adjusted by needs.

The model must be trained before it could be used, we web scraped 4448 Wikipedia articles as training data, including 2816 articles in statistics, 1005 articles in sociology and 627 in computing. The initial codes and functions to build the LDA model was provided by Dr Tanaka,  we have tested model outputs using different number of topics and tried out training the LDA models with different combinations of data.

```{r setup, include = FALSE}
library(tidyverse)
library(kableExtra)
library(patchwork)

knitr::opts_chunk$set(echo = FALSE,
                      warning = FALSE,
                      message= FALSE)
theme_set(theme_minimal())

rmv_col <- function(x){
  read_csv(x) %>% 
  select(-`...1`)
}

colors <- RColorBrewer::brewer.pal(10, "Paired")
names(colors) <- levels(factor(1:10))
my_scale <- scale_fill_manual(name = "topic", values = colors)  

data_s <- rmv_col("data/stats.csv")
data_ssc <- rmv_col("data/ssc.csv")
data_cs <- rmv_col("data/cs.csv")
data_comp <- rmv_col("data/comp.csv")
mod_sum <- rmv_col("data/modelsum_ssc.csv")
```

The table below shows a glimpse of the model output of the initial LDA model, after training by data collected from all 4448 Wikipedia articles on 12 topics.

::: {.content-visible unless-format="pdf"}
```{r}
mod_sum %>% 
  head(120) %>% 
  kable(align = "c") %>% 
  kable_styling(bootstrap_options = "condensed",
                position = "center") %>% 
  scroll_box(height = "420px")
```
:::

::: {.content-hidden unless-format="pdf"}
```{r}
mod_sum %>% 
  head(5) %>% 
  kable(align = "c") %>% 
  kable_styling(latex_options = c("striped", "scale_down"),
                position = "center")
```
:::

Term shows all the word extracted from the training data (Wikipedia articles), as each latent topic has a multinomial distribution for all words, beta value of a term is the score / probability computed for that particular topic. Highest beta value indicates highest probability, which means the term is most likely belongs to the corresponding topic. 

Beta values for the same word would differ from model to model, and also subject to change by adjusting the number of latent topics. To acquire the most satisfactory results for our university data, we have fitted and tested multiple LDA models with different subset of training data, as well as various number of topics. 


## Model Adjustments

After applying the fitted LDA models to our university data set, the results delivered by the models are quite different. @fig-com compares the results produced by the four fitted models using different training data on ten topics.

```{r}
ssc <- data_ssc %>% 
  mutate(topic = as_factor(topic)) %>%
  mutate(topic = recode_factor(topic,
                               "1" = "A1",
                               "2" = "A2",
                               "3" = "A3",
                               "4" = "A4",
                               "5" = "A5",
                               "6" = "A6",
                               "7" = "A7",
                               "8" = "A8",
                               "9" = "A9",
                               "10" = "A10")) %>% 
  group_by(School) %>% 
  count(topic, sort = TRUE) %>%
  mutate(prop = round(n/sum(n),4)) %>% 
  ungroup() %>% 
  mutate(topic = fct_reorder(topic, prop, sum))%>% 
  ggplot(aes(x = prop,
             y = topic))+
  geom_col(fill = "#1F78B4",
           colour = "white") +
  #my_scale +
  facet_wrap(~School, scales = "free_y") +
  theme(legend.position = "none") +
  labs(x = "",
       title = "Model A",
       subtitle = "Trained by sociology, statistics and computing data")

cs <- data_cs %>% 
  mutate(topic = as_factor(topic)) %>% 
  mutate(topic = recode_factor(topic,
                               "1" = "B1",
                               "2" = "B2",
                               "3" = "B3",
                               "4" = "B4",
                               "5" = "B5",
                               "6" = "B6",
                               "7" = "B7",
                               "8" = "B8",
                               "9" = "B9",
                               "10" = "B10")) %>% 
  group_by(School) %>% 
  count(topic, sort = TRUE) %>%
  mutate(prop = round(n/sum(n),4)) %>% 
  ungroup() %>% 
  mutate(topic = fct_reorder(topic, prop, sum))%>% 
  ggplot(aes(x = prop,
             y = topic,
             fill = topic))+
  geom_col(fill = "#33A02C",
           colour = "white") +
  #my_scale +
  facet_wrap(~School, scales = "free_y") +
  theme(legend.position = "none") +
  labs(x = "",
       title = "Model B",
       subtitle = "Trained by statistics and computing data")

comp <- data_comp %>% 
  mutate(topic = as_factor(topic)) %>% 
  mutate(topic = recode_factor(topic,
                               "1" = "C1",
                               "2" = "C2",
                               "3" = "C3",
                               "4" = "C4",
                               "5" = "C5",
                               "6" = "C6",
                               "7" = "C7",
                               "8" = "C8",
                               "9" = "C9",
                               "10" = "C10")) %>% 
  group_by(School) %>% 
  count(topic, sort = TRUE) %>%
  mutate(prop = round(n/sum(n),4)) %>% 
  ungroup() %>% 
  mutate(topic = fct_reorder(topic, prop, sum))%>% 
  ggplot(aes(x = prop,
             y = topic,
             fill = topic))+
  geom_col(fill = "#CAB2D6",
           colour = "white") +
  #my_scale +
  facet_wrap(~School, scales = "free_y") +
  theme(legend.position = "none") +
  labs(x = "proportion",
       title = "Model C",
       subtitle = "Trained by only computing data")

stats <- data_s %>% 
  mutate(topic = as_factor(topic)) %>% 
  mutate(topic = recode_factor(topic,
                               "1" = "D1",
                               "2" = "D2",
                               "3" = "D3",
                               "4" = "D4",
                               "5" = "D5",
                               "6" = "D6",
                               "7" = "D7",
                               "8" = "D8",
                               "9" = "D9",
                               "10" = "D10")) %>% 
  group_by(School) %>% 
  count(topic, sort = TRUE) %>%
  mutate(prop = round(n/sum(n),4)) %>% 
  ungroup() %>% 
  mutate(topic = fct_reorder(topic, prop, sum))%>% 
  ggplot(aes(x = prop,
             y = topic,
             fill = topic))+
  geom_col(fill = "#FF7F00",
           colour = "white") +
  #my_scale +
  facet_wrap(~School, scales = "free_y") +
  theme(legend.position = "none") +
  labs(x = "proportion",
       y = "",
       title = "Model D",
       subtitle = "Trained by only statistics data")

fig_comp <- (ssc + cs) / (comp + stats)
```

```{r fig-com, fig.cap = "Model D Delivers the Most Informative Results on University Data", fig.width=12, fig.height=12}
fig_comp +
  plot_annotation(title = "Model D Delivers the Most Informative Results on University Data")
```

For Model A, Topics A9, A10, A2 and A8 occupies relatively higher proportion compare with the others, but the order varies across universities, and their proportions are not significantly larger than the rest of other topics, makes it hard to draw meaningful interpretations. On the top right, Model B demonstrates a better picture: Topics B6, B8 and B9 are the top 3 topics across all Go8 universities, however, proportions of Topics B10, B5, B7, B3 and B4 are also obvious higher in some of the universities, brings in difficulties to make justifications.     

As sociology data tends to brings in noises to the model, and is not closely relevant to the data science topic compare with statistics and computing, Model C and D are fitted using only statistics data and computing data respectively. Topic C2 is the only dominating topic in Model C, where as Topics D6 and D1 occupy significantly large proportion in Model D. Besides, cccccc together took a relatively higher proportion compare with the rest of other topics in Model D.

The table below listed the top 30 words of each topics in Model C, it turns out Topic C2 contains words like comput (computation, computational, computer), system, program, machin (machine), softwar (software), model, test, calcul (calculate, calculation) and data, which seems to be associated with mainly computational aspects. 

::: {.content-visible unless-format="pdf"}
```{r}
rmv_col("data/term_comp30.csv") %>% 
  rename("Topic C1" = `Topic 1`,
         "Topic C2" = `Topic 2`,
         "Topic C3" = `Topic 3`,
         "Topic C4" = `Topic 4`,
         "Topic C5" = `Topic 5`,
         "Topic C6" = `Topic 6`,
         "Topic C7" = `Topic 7`,
         "Topic C8" = `Topic 8`,
         "Topic C9" = `Topic 9`,
         "Topic C10" = `Topic 10`) %>% 
  kable() %>% 
  kable_styling(bootstrap_options = "condensed") %>% 
  scroll_box(width = "1250px",
             height = "350px")
```
:::

::: {.content-hidden unless-format="pdf"}
```{r}
rmv_col("data/term_comp30.csv") %>% 
  rename("Topic C1" = `Topic 1`,
         "Topic C2" = `Topic 2`,
         "Topic C3" = `Topic 3`,
         "Topic C4" = `Topic 4`,
         "Topic C5" = `Topic 5`,
         "Topic C6" = `Topic 6`,
         "Topic C7" = `Topic 7`,
         "Topic C8" = `Topic 8`,
         "Topic C9" = `Topic 9`,
         "Topic C10" = `Topic 10`) %>% 
  kable() %>% 
  kable_styling(bootstrap_options = c("striped", "scale_down")) 
```
:::

Although Model D provides a reasonably meaningful result, there is not much interpretations could be made for the other topics, the information it offers is still not very satisfying. Compares with Model C, Model D has two domination topics D6 and D1, topics D5, D10, D2, D9 also accounts for a large proportion, which together provides more information. Therefore, Model D, which is trained by only statistics data on ten topics, is selected to use for further analysis on our university data.

Note that it requires highly skilled linguists and huge efforts to establish a proper text corpus, the model we built is still fairly basic and could be further optimised by adjustments.
 
