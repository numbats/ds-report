{
  "hash": "08ae5af556894709cb5c2d3c425f367b",
  "result": {
    "markdown": "# text2vec\n\nAs mentioned in Section @sec-unit-bigram, since the collected university data is relatively small, to make more educated and meaningful interpretations, similar words shall be grouped together and counted by groups. This is usually computed using text corpus, which is a language resource consisting of a large and structured set of texts, since data science is a new term waiting to be defined, there is no available text corpus on this topic. Therefore, we adopted the concept of word2vec and tried to build our own text corpus.\n\nThere are multiple publicly available models and packages to conduct similar computations, however, each model takes hours to fit. Due to time constrains, we have only fitted the Dirichlet Allocation (LDA) model with a few parameter adjustments using the `text2vec` package with the concepts illustrated by @das2016data. \n\n## Algorithm and Model Fitting\n\nAccording to @das2016data, the algorithm behind the LDA model is to convert words to document-term matrix (DTM), where the rows, columns and entries correspond to documents, terms and counts respectively. LDA then fits a probabilistic model that assumes a mixture of latent topics, where each topic has a multinomial distribution for all words. The number of topics is a parameter that could be adjusted by needs.\n\nThe initial code to build the LDA model was provided by Professor Tanaka, the major part of the code to build the first version of LDA model is also provided below.\n\n```{.r}\nlist(\n  tar_target(wiki_stats, get_wiki_articles(\"https://en.wikipedia.org/wiki/List_of_statistics_articles\")),\n  tar_target(wiki_sociology, get_wiki_articles(\"https://en.wikipedia.org/wiki/Index_of_sociology_articles\")),\n  tar_target(wiki_computing, get_wiki_articles(\"https://en.wikipedia.org/wiki/Index_of_computing_articles\")),\n  tar_target(clean_wiki_stats, map(wiki_stats, clean_wiki_article), format = \"rds\", repository = \"local\"),\n  tar_target(clean_wiki_sociology, map(wiki_sociology, clean_wiki_article), format = \"rds\", repository = \"local\"),\n  tar_target(clean_wiki_computing, map(wiki_computing, clean_wiki_article), format = \"rds\", repository = \"local\"),\n  tar_target(clean_stats, preprocess_text(clean_wiki_stats)),\n  \n  tar_target(clean_ssc, preprocess_text(c(clean_wiki_stats, clean_wiki_sociology, clean_wiki_computing))),\n  \n  tar_target(itoken_ssc, itoken(clean_ssc, tokenizer = stem_tokenizer),\n             cue = tar_cue(mode = \"thorough\")),\n \n  tar_target(vocab_ssc, create_vocabulary(itoken_ssc, ngram = c(1, 3), stopwords = stopwords::stopwords()),\n             cue = tar_cue(mode = \"thorough\")),\n  tar_target(vocab_ssc_prune, prune_vocab(vocab_ssc, n_min = 40),\n             cue = tar_cue(mode = \"thorough\")),\n  tar_target(dtm_ssc, create_dtm(itoken_ssc, vocab_vectorizer(vocab_ssc_prune)),\n             cue = tar_cue(mode = \"thorough\")),\n  tar_target(tcm_ssc, create_tcm(itoken_ssc, vocab_vectorizer(vocab_ssc_prune), \n                                 skip_grams_window = 5L),\n             cue = tar_cue(mode = \"thorough\")),\n  tar_target(word2vec_model_ssc, model_glove(vocab_ssc_prune, tcm_ssc),\n             cue = tar_cue(mode = \"thorough\")),\n  tar_target(word2vec_dist_ssc, dist2(t(word2vec_model_ssc$components), method = \"cosine\"),\n             cue = tar_cue(mode = \"thorough\"), format = \"rds\", repository = \"local\"),\n  tar_target(word2vec_res, find_close_words(\"statistics\", word2vec_dist_ssc, 10),\n             cue = tar_cue(mode = \"thorough\")),\n  tar_target(lda_model03_ssc, model_lda(dtm_ssc, ntopics = 3),\n             format = \"rds\", repository = \"local\"),\n  tar_target(lda_model20_ssc, model_lda(dtm_ssc, ntopics = 20),\n             format = \"rds\", repository = \"local\")\n             )\n```\n\nThe model must be trained before it could be used, we web scraped over 4448 Wikipedia articles as training data, including 2816 articles in statistics, 1005 articles in sociology and 627 in computing. The functions used in the codes above such as `get_wiki_articles`,`clean_wiki_article`, `get_clean_combined_wikis`,  `model_lda`, `preprocess_text`, `stem_tokenizer`, `prune_vocab`, `model_glove` and `find_close_words` are constructed by Professor Tanaka for pre-processing purposes, the original scripts could be found from the [project repository](https://github.com/numbats/datasci-courses).\n\n\n## Model Adjustments\n\nWe have tested using different values for parameter `ntopics` and tried out training the LDA models with different combinations of data. \n\nThe results provided differs from models, @fig-ssc-cs compares the results produced by the full model and model without sociology data on ten topics.\n\n\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![Compare Results Between Full Model and Without Sociology Data](04-text2vec_files/figure-html/fig-ssc-cs-1.png){#fig-ssc-cs width=960}\n:::\n:::\n\n\nFrom the results computed by the full model, Topics 9, 10, 2 and 8 occupies relatively higher proportion compare with the others, but the order varies across universities, and their proportions are not significantly larger than the rest of other topics, makes it hard to draw meaningful interpretations. On the right hand side, results from the model without sociology data demonstrates a better picture: Topics 6, 8 and 9 in this case are the top 3 topics across all Go8 universities, however, proportions of Topic 10, 5, 7, 3 and 4 are also obvious higher in some of the universities, brings in difficulties to make justifications.     \n\nAs sociology data tends to brings in noises to the model, and is not closely relevant to the data science topic compare with statistics and computing, another two models are fitted using only statistics data and computing data respectively, the results of both models are shown in @fig-comp-stats.\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n::: {.cell-output-display}\n![Compare Results Between Full Model and Without Sociology Data](04-text2vec_files/figure-html/fig-comp-stats-1.png){#fig-comp-stats width=960}\n:::\n:::\n\n\nTopic 2 is the only dominating topic based on the results provided by the model using only computing data, which provides a clearer picture than the previous models. The table below listed the top ten words for each topics, it turns out Topic 2 contains words like comput (computation etc.), system, program, softwar (software), which are associated with computational aspects, especially software. This model provides a more meaningful results than the prior ones, however, there is not much interpretations could be made for the other topics, the information it offers is still not very satisfying.\n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<div style=\"border: 1px solid #ddd; padding: 5px; overflow-x: scroll; width:1250px; \"><table class=\"table table-condensed\" style=\"margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> Topic 1 </th>\n   <th style=\"text-align:left;\"> Topic 2 </th>\n   <th style=\"text-align:left;\"> Topic 3 </th>\n   <th style=\"text-align:left;\"> Topic 4 </th>\n   <th style=\"text-align:left;\"> Topic 5 </th>\n   <th style=\"text-align:left;\"> Topic 6 </th>\n   <th style=\"text-align:left;\"> Topic 7 </th>\n   <th style=\"text-align:left;\"> Topic 8 </th>\n   <th style=\"text-align:left;\"> Topic 9 </th>\n   <th style=\"text-align:left;\"> Topic 10 </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> window </td>\n   <td style=\"text-align:left;\"> comput </td>\n   <td style=\"text-align:left;\"> ibm </td>\n   <td style=\"text-align:left;\"> algorithm </td>\n   <td style=\"text-align:left;\"> network </td>\n   <td style=\"text-align:left;\"> bit </td>\n   <td style=\"text-align:left;\"> format </td>\n   <td style=\"text-align:left;\"> intel </td>\n   <td style=\"text-align:left;\"> softwar </td>\n   <td style=\"text-align:left;\"> languag </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> system </td>\n   <td style=\"text-align:left;\"> system </td>\n   <td style=\"text-align:left;\"> comput </td>\n   <td style=\"text-align:left;\"> can </td>\n   <td style=\"text-align:left;\"> use </td>\n   <td style=\"text-align:left;\"> instruct </td>\n   <td style=\"text-align:left;\"> use </td>\n   <td style=\"text-align:left;\"> chip </td>\n   <td style=\"text-align:left;\"> compani </td>\n   <td style=\"text-align:left;\"> program </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> version </td>\n   <td style=\"text-align:left;\"> program </td>\n   <td style=\"text-align:left;\"> system </td>\n   <td style=\"text-align:left;\"> number </td>\n   <td style=\"text-align:left;\"> can </td>\n   <td style=\"text-align:left;\"> memori </td>\n   <td style=\"text-align:left;\"> imag </td>\n   <td style=\"text-align:left;\"> design </td>\n   <td style=\"text-align:left;\"> appl </td>\n   <td style=\"text-align:left;\"> use </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> releas </td>\n   <td style=\"text-align:left;\"> use </td>\n   <td style=\"text-align:left;\"> disk </td>\n   <td style=\"text-align:left;\"> function </td>\n   <td style=\"text-align:left;\"> web </td>\n   <td style=\"text-align:left;\"> use </td>\n   <td style=\"text-align:left;\"> digit </td>\n   <td style=\"text-align:left;\"> processor </td>\n   <td style=\"text-align:left;\"> free </td>\n   <td style=\"text-align:left;\"> compil </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> support </td>\n   <td style=\"text-align:left;\"> machin </td>\n   <td style=\"text-align:left;\"> drive </td>\n   <td style=\"text-align:left;\"> set </td>\n   <td style=\"text-align:left;\"> data </td>\n   <td style=\"text-align:left;\"> address </td>\n   <td style=\"text-align:left;\"> can </td>\n   <td style=\"text-align:left;\"> bit </td>\n   <td style=\"text-align:left;\"> use </td>\n   <td style=\"text-align:left;\"> code </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> user </td>\n   <td style=\"text-align:left;\"> design </td>\n   <td style=\"text-align:left;\"> use </td>\n   <td style=\"text-align:left;\"> state </td>\n   <td style=\"text-align:left;\"> internet </td>\n   <td style=\"text-align:left;\"> regist </td>\n   <td style=\"text-align:left;\"> video </td>\n   <td style=\"text-align:left;\"> core </td>\n   <td style=\"text-align:left;\"> also </td>\n   <td style=\"text-align:left;\"> function </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> oper_system </td>\n   <td style=\"text-align:left;\"> process </td>\n   <td style=\"text-align:left;\"> control </td>\n   <td style=\"text-align:left;\"> languag </td>\n   <td style=\"text-align:left;\"> secur </td>\n   <td style=\"text-align:left;\"> processor </td>\n   <td style=\"text-align:left;\"> standard </td>\n   <td style=\"text-align:left;\"> mhz </td>\n   <td style=\"text-align:left;\"> develop </td>\n   <td style=\"text-align:left;\"> object </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> oper </td>\n   <td style=\"text-align:left;\"> develop </td>\n   <td style=\"text-align:left;\"> machin </td>\n   <td style=\"text-align:left;\"> use </td>\n   <td style=\"text-align:left;\"> access </td>\n   <td style=\"text-align:left;\"> oper </td>\n   <td style=\"text-align:left;\"> disc </td>\n   <td style=\"text-align:left;\"> use </td>\n   <td style=\"text-align:left;\"> open </td>\n   <td style=\"text-align:left;\"> type </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> os </td>\n   <td style=\"text-align:left;\"> inform </td>\n   <td style=\"text-align:left;\"> card </td>\n   <td style=\"text-align:left;\"> symbol </td>\n   <td style=\"text-align:left;\"> protocol </td>\n   <td style=\"text-align:left;\"> can </td>\n   <td style=\"text-align:left;\"> data </td>\n   <td style=\"text-align:left;\"> introduc </td>\n   <td style=\"text-align:left;\"> sourc </td>\n   <td style=\"text-align:left;\"> can </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> file </td>\n   <td style=\"text-align:left;\"> softwar </td>\n   <td style=\"text-align:left;\"> unit </td>\n   <td style=\"text-align:left;\"> problem </td>\n   <td style=\"text-align:left;\"> link </td>\n   <td style=\"text-align:left;\"> data </td>\n   <td style=\"text-align:left;\"> file </td>\n   <td style=\"text-align:left;\"> microprocessor </td>\n   <td style=\"text-align:left;\"> user </td>\n   <td style=\"text-align:left;\"> implement </td>\n  </tr>\n</tbody>\n</table></div>\n\n`````\n:::\n:::\n\n\nIn terms of the model trained by only data in statistics, there are also dominating topics across all eight universities: Topics 6 and 1, besides, topics 5, 10, 2, 9 together took a relatively higher proportion compare with the rest of other topics. Both models using only computing or statistical data delivers better results, model trained by only statistical data provides more information than the other, hence is selected to use for further analysis on our university data.\n\nNote that it requires highly skilled linguists and huge efforts to establish a proper text corpus, the model we built is still fairly basic and could be further optimised by adjustments.\n \n",
    "supporting": [
      "04-text2vec_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}