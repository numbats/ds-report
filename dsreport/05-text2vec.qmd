# text2vec

As mentioned in Section @sec-unit-bigram, since the collected university data is relatively small, to make more educated and meaningful interpretations, similar words shall be grouped together and counted by groups. This is usually computed using text corpus, which is a language resource consisting of a large and structured set of texts, since data science is a new term waiting to be defined, there is no available text corpus on this topic. Therefore, we adopted the concept of word2vec and tried to build our own text corpus.

There are multiple publicly available models and packages to conduct similar computations, however, each model takes hours to fit. Due to time constrains, we have only fitted the Dirichlet Allocation (LDA) model with a few parameter adjustments using the `text2vec` package with the concepts illustrated by @das2016data. 

## Algorithm and Model Fitting

According to @das2016data, the algorithm behind the LDA model is to convert words to document-term matrix (DTM), where the rows, columns and entries correspond to documents, terms and counts respectively. LDA then fits a probabilistic model that assumes a mixture of latent topics, where each topic has a multinomial distribution for all words. The number of topics is a parameter that could be adjusted by needs.

The initial code to build the LDA model was provided by Professor Tanaka, the major part of the code to build the first version of LDA model is also provided below.

```{.r}
list(
  tar_target(wiki_stats, get_wiki_articles("https://en.wikipedia.org/wiki/List_of_statistics_articles")),
  tar_target(wiki_sociology, get_wiki_articles("https://en.wikipedia.org/wiki/Index_of_sociology_articles")),
  tar_target(wiki_computing, get_wiki_articles("https://en.wikipedia.org/wiki/Index_of_computing_articles")),
  tar_target(clean_wiki_stats, map(wiki_stats, clean_wiki_article), format = "rds", repository = "local"),
  tar_target(clean_wiki_sociology, map(wiki_sociology, clean_wiki_article), format = "rds", repository = "local"),
  tar_target(clean_wiki_computing, map(wiki_computing, clean_wiki_article), format = "rds", repository = "local"),
  tar_target(clean_stats, preprocess_text(clean_wiki_stats)),
  
  tar_target(clean_ssc, preprocess_text(c(clean_wiki_stats, clean_wiki_sociology, clean_wiki_computing))),
  
  tar_target(itoken_ssc, itoken(clean_ssc, tokenizer = stem_tokenizer),
             cue = tar_cue(mode = "thorough")),
 
  tar_target(vocab_ssc, create_vocabulary(itoken_ssc, ngram = c(1, 3), stopwords = stopwords::stopwords()),
             cue = tar_cue(mode = "thorough")),
  tar_target(vocab_ssc_prune, prune_vocab(vocab_ssc, n_min = 40),
             cue = tar_cue(mode = "thorough")),
  tar_target(dtm_ssc, create_dtm(itoken_ssc, vocab_vectorizer(vocab_ssc_prune)),
             cue = tar_cue(mode = "thorough")),
  tar_target(tcm_ssc, create_tcm(itoken_ssc, vocab_vectorizer(vocab_ssc_prune), 
                                 skip_grams_window = 5L),
             cue = tar_cue(mode = "thorough")),
  tar_target(word2vec_model_ssc, model_glove(vocab_ssc_prune, tcm_ssc),
             cue = tar_cue(mode = "thorough")),
  tar_target(word2vec_dist_ssc, dist2(t(word2vec_model_ssc$components), method = "cosine"),
             cue = tar_cue(mode = "thorough"), format = "rds", repository = "local"),
  tar_target(word2vec_res, find_close_words("statistics", word2vec_dist_ssc, 10),
             cue = tar_cue(mode = "thorough")),
  tar_target(lda_model03_ssc, model_lda(dtm_ssc, ntopics = 3),
             format = "rds", repository = "local"),
  tar_target(lda_model20_ssc, model_lda(dtm_ssc, ntopics = 20),
             format = "rds", repository = "local")
             )
```

The model must be trained before it could be used, we web scrapped over 4448 Wikipedia articles as training data, including 2816 articles in statistics, 1005 articles in sociology and 627 in computing. The functions used in the codes above such as `get_wiki_articles`,`clean_wiki_article`, `get_clean_combined_wikis`,  `model_lda`, `preprocess_text`, `stem_tokenizer`, `prune_vocab`, `model_glove` and `find_close_words` are constructed by Professor Tanaka for pre-processing purposes, the original scripts could be found from the [project repository](https://github.com/numbats/datasci-courses).

We have later adjusted the parameter `ntopics` and tried out training the LDA models with different combination of data, upon checking, the sociology data does not help improve model performances, instead it brings in noises. The most informative results are provided by the LDA model using only the statistics data on 10 topics. It requires highly skilled linguists and huge efforts to establish a proper text corpus, the model we built is still fairly basic and could be further optimised by adjustments.

## Apply Fitted Model to Collected University Data

Before applying the fitted LDA model to our university data set, words from unit overview and learning outcomes are stemmed using the `SnowballC` package, so that noises like plurals and part of speech are removed. The stemmed words are then assigned to the corresponding topic with the highest probability, instead of counting the appearance of words, the new counts generated are based on topics.

Similar with the university breakdown in Section @sec-unit-bigram, to make more objective comparisons, counts are converted to proportions due to different number of units scrapped for the eight universities. @fig-unitopics suggests that Topics 1 and 6 are obviously the dominating topics in Master of data science at all Go8 universities, whereas Topics 2, 5, 9, 10 together also occupies a relatively large proportion. 

```{r message=FALSE, echo=FALSE}
library(tidyverse)
library(kableExtra)

rmv_col <- function(x){
  read_csv(x) %>% 
  select(-`...1`)
}

data_s <- rmv_col("data/stats.csv")

colors <- RColorBrewer::brewer.pal(10, "Paired")
names(colors) <- levels(factor(1:10))
my_scale <- scale_fill_manual(name = "topic", values = colors)   

```

```{r fig-unitopics, fig.cap = "Topics Proportion by University", echo=FALSE}
data_s %>% 
  mutate(topic = as_factor(topic)) %>% 
  #distinct(School, Unit, topic) %>% 
  group_by(School) %>% 
  count(topic, sort = TRUE) %>%
  mutate(prop = round(n/sum(n),4)) %>% 
  ungroup() %>% 
  mutate(topic = fct_reorder(topic, prop, sum))%>% 
  ggplot(aes(x = prop,
             y = topic,
             fill = topic))+
  geom_col() +
  my_scale +
  #scale_y_reordered() +
  facet_wrap(~School, scales = "free_y") +
  theme(legend.position = "none") +
  labs(x = "proportion",
       title = "Topics Proportion by University")
```

The top ten words based on probabilities for each of the ten topics are provided below, colours of the columns are aligned with @fig-unitopics. Topic 1 contains words like statist (statistics), data, popul (population), and we can see data, algorithm, analysi (analysis), model, cluster, comput (computation etc.) in topic 6, it is a reasonable interpretation that these two topics are both associated with computational aspects.

```{r message=FALSE, echo=FALSE}
rmv_col("data/term_s.csv") %>% 
  kable() %>% 
  kable_styling(bootstrap_options = "condensed") %>% 
  column_spec(1, color = "white",
               background = "#A6CEE3") %>% 
  column_spec(2, color = "white",
               background = "#1F78B4") %>% 
  column_spec(3, color = "white",
               background = "#B2DF8A")%>% 
  column_spec(4, color = "white",
               background = "#33A02C")%>% 
  column_spec(5, color = "white",
               background = "#FB9A99")%>% 
  column_spec(6, color = "white",
               background = "#E31A1C") %>% 
  column_spec(7, color = "white",
               background = "#FDBF6F")%>% 
  column_spec(8, color = "white",
               background = "#FF7F00")%>% 
  column_spec(9, color = "white",
               background = "#CAB2D6") %>%
  column_spec(10, color = "white",
               background = "#6A3D9A") %>% 
  scroll_box(width = "1250px")
```


In addition, words under Topics 2,5,9 and 10 are model, regression, estim (estimate), linear, probabl (probability), bayesian, prior, infer, test, statist (statistics), hypothesi (hypothesis), correl (correlation), most of them are related to math and statistics, and also more on the computational side of them. 

The results above further proves the earlier findings discussed in Section @sec-unit-code and Section @sec-unit-bigram: Master of Data Science degrees offered at Go8 universities tend to be mainly IT based, the major components are computational as well as statistical/mathematical aspects. 

```{r fig-unitopics2, fig.cap = "Proportion Breakdown by Topic", echo=FALSE}
data_s %>% 
  mutate(topic = as_factor(topic)) %>% 
  #distinct(School, Unit, topic) %>% 
  group_by(School) %>% 
  count(topic, sort = TRUE) %>%
  mutate(prop = round(n/sum(n),4)) %>% 
  ungroup() %>% 
  mutate(topic = fct_reorder(topic, prop, sum))%>% 
  ggplot(aes(x = prop,
             y = School,
             fill = School))+
  geom_col() +
  scale_fill_brewer(palette = "Paired") +
  #scale_y_reordered() +
  facet_wrap(~topic, scales = "free_y") +
  theme(legend.position = "none") +
  labs(x = "proportion",
       title = "Breakdown by Topic")
```


@fig-unitopics2 above demonstrates a breakdown by topics instead of universities, it is clear that compares with the results based on only faculty in @sec-unit-code, the differences between Go8 are not as much here. The proportions occupied by the eight universities under each topic are fairly similar to each other, indicating the subjective choice made regarding the grouping method in Section @sec-unit-code might have provided a slightly misleading information, but it would require further explorations to confirm whether it is truly the case.

 
